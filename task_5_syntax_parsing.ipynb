{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# chapter 5: Dependency parsing\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yx0xiG03en4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 40. Read the parse result (words)\n",
        "Design a class Word that represents a word. This class has three member variables, text (word surface), lemma (lemma), and pos (part-of-speech). Represent a sentence as an array of instances of Word class. Implement a program to load the parse result, and store the text as an array of sentences. Show the object of the first sentence of the body of the article."
      ],
      "metadata": {
        "id": "0778Dk_AAISc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8LGj-lLeiHt",
        "outputId": "2cc251f4-4f2e-42dd-d14c-3abb567bb797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In (in, IN)\n",
            "computer (computer, NN)\n",
            "science (science, NN)\n",
            ", (,, ,)\n",
            "artificial (artificial, JJ)\n",
            "intelligence (intelligence, NN)\n",
            "-LRB- (-lrb-, -LRB-)\n",
            "AI (ai, NN)\n",
            "-RRB- (-rrb-, -RRB-)\n",
            ", (,, ,)\n",
            "sometimes (sometimes, RB)\n",
            "called (call, VBN)\n",
            "machine (machine, NN)\n",
            "intelligence (intelligence, NN)\n",
            ", (,, ,)\n",
            "is (be, VBZ)\n",
            "intelligence (intelligence, NN)\n",
            "demonstrated (demonstrate, VBN)\n",
            "by (by, IN)\n",
            "machines (machine, NNS)\n",
            ", (,, ,)\n",
            "in (in, IN)\n",
            "contrast (contrast, NN)\n",
            "to (to, TO)\n",
            "the (the, DT)\n",
            "natural (natural, JJ)\n",
            "intelligence (intelligence, NN)\n",
            "displayed (display, VBN)\n",
            "by (by, IN)\n",
            "humans (human, NNS)\n",
            "and (and, CC)\n",
            "animals (animal, NNS)\n",
            ". (., .)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "class Word:\n",
        "    def __init__(self, text, lemma, pos):\n",
        "        self.text = text\n",
        "        self.lemma = lemma\n",
        "        self.pos = pos\n",
        "\n",
        "def load_parse_result(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "    sentences = []\n",
        "    for sentence in data['sentences']:\n",
        "        words = [Word(word['word'], word['lemma'], word['pos']) for word in sentence['tokens']]\n",
        "        sentences.append(words)\n",
        "    return sentences\n",
        "\n",
        "# function call\n",
        "file_path = 'ai.en.txt.json'\n",
        "sentences = load_parse_result(file_path)\n",
        "\n",
        "# Print the first sentence to verify\n",
        "if sentences:\n",
        "    for word in sentences[0]:\n",
        "        print(f\"{word.text} ({word.lemma}, {word.pos})\")\n",
        "else:\n",
        "    print(\"No data found.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nupRS5RhIIOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 41. Read the parse result (dependency)\n",
        "In addition to problem 40, add three member variables head (a reference to the object of its syntactic governor), dep (dependency type to its governor), and children (a list of references to the syntactic dependents in the parse tree) to the class Word. Show the pairs of governors (parents) and their dependents (children) of the first sentence of the body of the article. Use the class Word in the rest of the problems in this chapter."
      ],
      "metadata": {
        "id": "IBNFutk3JRl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Word:\n",
        "    def __init__(self, text, lemma, pos, head=None, dep=None):\n",
        "        self.text = text\n",
        "        self.lemma = lemma\n",
        "        self.pos = pos\n",
        "        self.head = head\n",
        "        self.dep = dep\n",
        "        self.children = []\n",
        "\n",
        "def load_parse_result_with_dependencies(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "    sentences = []\n",
        "    for sentence in data['sentences']:\n",
        "        words = [Word(word['word'], word['lemma'], word['pos']) for word in sentence['tokens']]\n",
        "        for dep in sentence['basicDependencies']:\n",
        "            governor_idx = dep['governor'] - 1\n",
        "            dependent_idx = dep['dependent'] - 1\n",
        "            if governor_idx >= 0:\n",
        "                words[dependent_idx].head = words[governor_idx]\n",
        "                words[governor_idx].children.append(words[dependent_idx])\n",
        "            words[dependent_idx].dep = dep['dep']\n",
        "        sentences.append(words)\n",
        "    return sentences\n",
        "\n",
        "# function call\n",
        "sentences_with_dependencies = load_parse_result_with_dependencies(file_path)\n",
        "\n",
        "# Print governors and their dependents for the first sentence\n",
        "if sentences_with_dependencies:\n",
        "    for word in sentences_with_dependencies[0]:\n",
        "        if word.head:\n",
        "            print(f\"{word.head.text} ({word.head.pos}) -> {word.text} ({word.pos}) [{word.dep}]\")\n",
        "else:\n",
        "    print(\"No data found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axIXQ_pnJWj1",
        "outputId": "2ec016ff-f8f4-4ade-ac25-1b28b88e7ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "science (NN) -> In (IN) [case]\n",
            "science (NN) -> computer (NN) [compound]\n",
            "called (VBN) -> science (NN) [nmod]\n",
            "called (VBN) -> , (,) [punct]\n",
            "intelligence (NN) -> artificial (JJ) [amod]\n",
            "called (VBN) -> intelligence (NN) [nsubj]\n",
            "AI (NN) -> -LRB- (-LRB-) [punct]\n",
            "intelligence (NN) -> AI (NN) [appos]\n",
            "AI (NN) -> -RRB- (-RRB-) [punct]\n",
            "intelligence (NN) -> , (,) [punct]\n",
            "called (VBN) -> sometimes (RB) [advmod]\n",
            "intelligence (NN) -> machine (NN) [compound]\n",
            "called (VBN) -> intelligence (NN) [xcomp]\n",
            "called (VBN) -> , (,) [punct]\n",
            "called (VBN) -> is (VBZ) [advcl]\n",
            "is (VBZ) -> intelligence (NN) [nsubj]\n",
            "intelligence (NN) -> demonstrated (VBN) [acl]\n",
            "machines (NNS) -> by (IN) [case]\n",
            "demonstrated (VBN) -> machines (NNS) [nmod]\n",
            "intelligence (NN) -> , (,) [punct]\n",
            "contrast (NN) -> in (IN) [case]\n",
            "intelligence (NN) -> contrast (NN) [nmod]\n",
            "intelligence (NN) -> to (TO) [case]\n",
            "intelligence (NN) -> the (DT) [det]\n",
            "intelligence (NN) -> natural (JJ) [amod]\n",
            "contrast (NN) -> intelligence (NN) [nmod]\n",
            "intelligence (NN) -> displayed (VBN) [acl]\n",
            "humans (NNS) -> by (IN) [case]\n",
            "displayed (VBN) -> humans (NNS) [nmod]\n",
            "humans (NNS) -> and (CC) [cc]\n",
            "humans (NNS) -> animals (NNS) [conj]\n",
            "called (VBN) -> . (.) [punct]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 42. Show root words\n",
        "For each sentence, extract the root word (whose head is ROOT)"
      ],
      "metadata": {
        "id": "zry3PqXNJteF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_root_words(sentences):\n",
        "  root_words = []\n",
        "  for sentence in sentences:\n",
        "    for word in sentence:\n",
        "      if word.head is None: #root word has no head\n",
        "        root_words.append(word)\n",
        "  return root_words\n",
        "\n",
        "\n",
        "# function call\n",
        "root_words = extract_root_words(sentences_with_dependencies)\n",
        "root_words = [word.text  for word in root_words]\n",
        "print(root_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeUctpLWJzIz",
        "outputId": "e0b8f640-6637-488c-c58e-153254c32908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['called', 'define', 'used', 'removed', 'says', 'excluded', 'classified', 'founded', 'divided', 'based', 'based', 'include', 'goals', 'include', 'used', 'draws', 'founded', 'raises', 'explored', 'consider', 'believe', 'experienced', 'appeared', 'raised', 'study', 'led', 'known', 'led', 'Turing', 'work', 'born', 'became', 'produced', 'established', 'optimistic', 'agreed', 'failed', 'slowed', 'called', 'revived', 'reached', 'inspired', 'fell', 'enabled', 'book', 'began', 'due', 'became', 'defeated', 'enabled', 'uses', 'won', 'won', 'marked', 'year', 'presents', 'attributes', 'include', 'reported', 'accelerated', 'acknowledged', 'defines', 'characterizes', 'analyzes', 'simple', 'defined', 'induced', 'induce', 'systems', 'benchmarked', 'revolves', 'set', 'built', 'recipe', 'capable', 'theoretically', 'derive', 'possible', 'involves', 'skip', 'symbolism', 'inference', 'analogizers', 'harder', 'overlap', 'use', 'work', 'obvious', 'nuanced', 'work', 'designed', 'Settling', 'attempt', 'disappoint', 'is', 'is', 'results', 'lacks', 'enables', 'have', 'councilmen', 'means', 'reason', 'limited', 'come', 'straightforward', 'gives', 'aim', 'refers', 'is', 'broken', 'consist', 'received', 'developed', 'developed', 'proved', 'use', 'solve', 'central', 'systems', 'attempt', 'contain', 'ontology', 'captured', 'called', 'used', 'are', 'able', 'need', 'assume', 'requires', 'calls', 'uses', 'used', 'study', 'ability', 'includes', 'used', 'attempt', 'viewed', 'assess', 'rewarded', 'uses', 'gives', 'enable', 'include', 'use', 'popular', '``', 'combine', 'scales', 'is', 'ability', 'include', 'ability', 'ambiguous', 'used', 'learn', 'determine', 'process', 'involves', 'generalizes', 'attributed', 'extended', 'remains', 'umbrella', 'include', 'valuable', 'allow', 'mimic', 'programmed', 'Historically', 'failed', 'work', 'predict', 'have', 'is', 'include', 'argue', 'look', 'require', 'require', 'considered', 'is', 'disagree', 'these', 'biology', 'described', 'require', 'explored', 'built', 'gathered', 'abandoned', 'began', 'centered', 'named', 'achieved', 'abandoned', 'convinced', 'studied', 'used', 'culminate', 'felt', 'focused', 'focus', 'found', 'described', 'example', 'began', 'led', 'base', 'driven', 'seemed', 'began', 'manage', 'includes', 'rejected', 'revived', 'coincided', 'elaborated', 'revived', 'example', 'include', 'studied', 'bogged', 'adopted', 'permitted', 'gaining', 'led', 'measurable', 'have', 'note', 'caution', 'developed', 'discussed', 'solved', 'viewed', 'search', 'use', 'use', 'sufficient', 'search', 'is', 'serve', 'supply', 'limit', 'came', 'possible', 'visualized', 'annealing', 'uses', 'begin', 'include', 'coordinate', 'optimization', 'used', 'method', 'used', 'involves', 'adds', 'assigns', 'used', 'fails', 'forms', 'designed', 'designed', 'require', 'devised', 'tool', 'used', 'expensive', 'independent', 'require', 'used', 'uses', 'utility', 'analysis', 'include', 'divided', 'do', 'functions', 'tuned', 'known', 'belongs', 'seen', 'known', 'received', 'trained', 'machine', 'network', 'depends', 'perform', 'available', 'inspired', 'accepts', 'requires', 'forms', 'have', 'learn', 'included', 'times', 'founded', 'invented', 'include', 'acyclic', 'are', 'applied', 'trained', 'approach', 'use', 'argue', 'is', 'network', 'learn', 'need', 'transformed', 'introduced', 'published', 'trained', 'describes', 'introduced', 'model', 'led', 'uses', 'applied', 'processed', 'won', 'used', 'applied', 'unlimited', 'trained', 'shown', 'use', 'trained', 'revolutionized', 'experienced', 'used', 'improved', 'technology', 'is', 'require', 'suggested', 'suggests', 'provide', 'brought', 'provide', 'continue', 'are', 'include', 'considered', 'test', 'helps', 'administered', 'asks', 'unable', 'test', 'aim', 'contain', 'relevant', 'pervasive', 'considered', 'include', 'overtaking', 'produce', 'reports', 'opens', 'used', 'increasing', 'applied', 'found', 'assisting', 'developed', 'is', 'are', 'affects', 'working', 'is', 'fighting', 'reported', 'using', 'done', 'demonstrated', 'claimed', 'created', 'struggled', 'contributed', 'are', 'include', 'contribute', 'incorporate', 'integrated', 'made', 'passed', 'fleet', 'testing', 'mapping', 'pre-programmed', 'include', 'working', 'equipped', 'safety', 'program', 'include', 'be', 'is', 'need', 'crucial', 'used', 'traced', 'using', 'use', 'react', 'beat', 'reduced', 'used', 'predicted', 'changed', 'buying', 'reduce', 'limits', 'include', '.', 'introduced', 'faces', 'begun', 'use', 'enables', 'consists', 'used', 'case', 'competed', 'set', 'involve', 'becoming', 'using', 'focused', 'vogue', 'used', 'used', 'consider', 'include', 'developing', 'are', 'underway', 'enable', 'incorporated', 'rose', 'considered', 'seek', 'needs', 'represented', 'makes', 'analyze', 'risk', 'possible', 'reports', 'help', 'inspired', 'Machines', 'include', 'dedicated', 'opened', 'thematized', 'are', 'intelligent', 'think', 'have', 'described', 'proposed', 'issue', 'expressed', 'cause', 'come', 'dangerous', 'governed', 'expressed', 'provides', 'argues', 'reflect', 'emphasizes', 'uses', 'argues', 'focused', 'led', 'committed', 'mixed', 'believe', 'stated', 'believes', 'donated', 'is', 'companies', 'think', 'have', 'revolve', 'wrote', 'bothered', 'suggest', 'is', 'higher', 'have', 'counts', 'suggested', 'complicated', 'creates', 'eliminated', 'vary', 'Jobs', 'go', 'point', 'researching', 'want', 'have', 'is', 'talks', 'introduced', 'centered', 'concerned', 'delineated', 'engaged', 'come', 'necessitate', 'concerned', 'key', 'enable', 'referred', 'found', 'believes', 'argues', 'assume', 'decide', 'begun', 'is', 'question', 'writes', 'think', 'sentient', 'related', 'identified', 'understanding', 'explaining', 'easy', 'consider', 'requires', 'is', 'Consider', 'knows', 'explaining', 'position', 'argues', 'inspired', 'position', 'counters', 'feel', 'have', 'considered', 'argue', 'discussed', 'be', 'agent', 'refer', 'able', 'better', 'increase', 'named', 'is', 'occurrence', 'used', 'predicted', 'has', 'argues', 'uncertain', 'showed', 'paper', 'considered', 'called', 'have', 'appeared', 'began', 'includes', 'prominent', 'introduced', 'brought', 'explored', 'painted', 'considered', 'use', 'appears', 'Dick', 'considers']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JA8aLLVkOdYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 43. Show verb governors and noun dependents\n",
        "Show all pairs of verb governors (parents) and their noun dependents (children) from all sentences in the text"
      ],
      "metadata": {
        "id": "8TpxHTlNOd_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_verb_governors_and_noun_dependents(sentences):\n",
        "    pairs = []\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            if word.pos.startswith('VB'):  # Verb governor\n",
        "                for child in word.children:\n",
        "                    if child.pos.startswith('NN'):  # Noun dependent\n",
        "                        pairs.append((word.text, child.text))\n",
        "    return pairs\n",
        "\n",
        "# Example usage\n",
        "verb_noun_pairs = extract_verb_governors_and_noun_dependents(sentences_with_dependencies)\n",
        "print(verb_noun_pairs[:10])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPiU8X6ZOmAM",
        "outputId": "e6c2d641-9bbf-44ee-b4e0-7b396600b44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('called', 'science'), ('called', 'intelligence'), ('called', 'intelligence'), ('is', 'intelligence'), ('demonstrated', 'machines'), ('displayed', 'humans'), ('define', 'textbooks'), ('define', 'field'), ('define', 'study'), ('perceives', 'environment')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 44. Visualize dependency trees\n",
        "Visualize a dependency tree of a sentence as a directed graph. Consider converting a dependency tree into DOT language and use Graphviz for drawing a directed graph. In addition, you can use pydot for drawing a dependency tree."
      ],
      "metadata": {
        "id": "jMql2A2DQagF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pydot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3EV86zJSnrS",
        "outputId": "d0bd9316-cf4b-4f04-b7fc-1b4ae66943e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydot in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pydot) (3.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pydot\n",
        "import re\n",
        "\n",
        "def escape_dot_label(label):\n",
        "    \"\"\"Escape special characters in DOT label.\"\"\"\n",
        "    return re.sub(r'([<>\\\\])', r'\\\\\\1', label)\n",
        "\n",
        "def visualize_dependency_tree(sentence, file_name='dependency_tree.png'):\n",
        "    graph = pydot.Dot(graph_type='digraph')\n",
        "\n",
        "    nodes = {}\n",
        "    for word in sentence:\n",
        "        escaped_text = escape_dot_label(word.text)\n",
        "        if word.text not in nodes:\n",
        "            node = pydot.Node(escaped_text)\n",
        "            nodes[word.text] = node\n",
        "            graph.add_node(node)\n",
        "        if word.head and word.head.text:\n",
        "            escaped_head = escape_dot_label(word.head.text)\n",
        "            if word.head.text not in nodes:\n",
        "                head_node = pydot.Node(escaped_head)\n",
        "                nodes[word.head.text] = head_node\n",
        "                graph.add_node(head_node)\n",
        "            edge = pydot.Edge(nodes[word.head.text], nodes[word.text], label=escape_dot_label(word.dep))\n",
        "            graph.add_edge(edge)\n",
        "\n",
        "    # graph.write_png(file_name)\n",
        "\n",
        "# function call\n",
        "if sentences_with_dependencies:\n",
        "    visualize_dependency_tree(sentences_with_dependencies[0], 'first_sentence_tree.png')\n"
      ],
      "metadata": {
        "id": "XnAjfgw0Qgl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 45. Triple with subject, verb, and direct objec\n",
        "We are interested in extracting facts from the text. In this chapter, we represent a fact as a tuple of (subject, predicate, object). Extract tuples from dependency trees where:\n",
        "\n",
        "subject is a nominal subject of a verb in the past tense\n",
        "predicate is the verb in the past tense\n",
        "object is a direct object of the verb\n",
        "Consider an example sentence, “Frank Rosenblatt invented the perceptron”. We want to extract a tuple, (Rosenblatt, invented, perceptron), from the sentence. In this problem, we only consider a subject and object as a single word."
      ],
      "metadata": {
        "id": "hEsKlyQcTd7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_svo_triples(sentences):\n",
        "    triples = []\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            if word.pos == 'VBD':  # Verb in past tense\n",
        "                subject = None\n",
        "                obj = None\n",
        "                for child in word.children:\n",
        "                    if child.dep == 'nsubj':\n",
        "                        subject = child\n",
        "                    elif child.dep == 'dobj':\n",
        "                        obj = child\n",
        "                if subject and obj:\n",
        "                    triples.append((subject.text, word.text, obj.text))\n",
        "    return triples\n",
        "\n",
        "# function call\n",
        "svo_triples = extract_svo_triples(sentences_with_dependencies)\n",
        "print(svo_triples[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGHIpGCaTrqb",
        "outputId": "116ddc8a-f1c8-419a-e53c-110a45e6af13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('characters', 'raised', 'many'), ('this', 'led', 'researchers'), ('They', 'produced', 'programs'), ('governments', 'cut', 'research'), ('project', 'inspired', 'U.S'), ('development', 'enabled', 'development'), ('match', 'defeated', 'champions'), ('computers', 'enabled', 'advances'), ('AlphaGo', 'won', 'games'), ('AlphaGo', 'won', 'match')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 46. Expanding subjects and objects\n",
        "Improve the program of Problem 45 to remove the restriction that subjects and objects are single words but can also be phrases. For example, we want to extract (Frank Rosenblatt, invented, perceptron) from the sentence, “Frank Rosenblatt invented the perceptron”."
      ],
      "metadata": {
        "id": "n3uRjoYdT5EC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_expanded_svo_triples(sentences):\n",
        "    triples = []\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            if word.pos == 'VBD':  # Verb in past tense\n",
        "                subject = []\n",
        "                obj = []\n",
        "                for child in word.children:\n",
        "                    if child.dep == 'nsubj':\n",
        "                        subject.append(child.text)\n",
        "                        for grandchild in child.children:\n",
        "                            if grandchild.dep in ['compound', 'amod']:\n",
        "                                subject.append(grandchild.text)\n",
        "                    elif child.dep == 'dobj':\n",
        "                        obj.append(child.text)\n",
        "                        for grandchild in child.children:\n",
        "                            if grandchild.dep in ['compound', 'amod']:\n",
        "                                obj.append(grandchild.text)\n",
        "                if subject and obj:\n",
        "                    triples.append((' '.join(subject), word.text, ' '.join(obj)))\n",
        "    return triples\n",
        "\n",
        "# function call\n",
        "expanded_svo_triples = extract_expanded_svo_triples(sentences_with_dependencies)\n",
        "print(expanded_svo_triples[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkvJumARUjqN",
        "outputId": "ae03004a-eea8-4a71-97dc-bae7ebe04b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('characters', 'raised', 'many'), ('this', 'led', 'researchers'), ('They', 'produced', 'programs'), ('governments U.S.', 'cut', 'research exploratory'), ('project fifth generation computer', 'inspired', 'U.S'), ('development', 'enabled', 'development'), ('match Jeopardy! quiz show exhibition', 'defeated', 'champions greatest Jeopardy!'), ('computers Faster', 'enabled', 'advances'), ('AlphaGo', 'won', '4 games'), ('AlphaGo', 'won', 'match three-game')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 47. Triple from the passive sentence\n",
        "Extract facts from sentences in the passive voice. Consider an example sentence, “Artificial intelligence was founded as an academic discipline in 1955”. We want to extract two tuples from the sentence,\n",
        "\n",
        "(Artificial intelligence, founded-as, academic discipline)\n",
        "(Artificial intelligence, founded-in, 1955)"
      ],
      "metadata": {
        "id": "W4cpjyaxVRND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_passive_triples(sentences):\n",
        "    triples = []\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            if word.dep == 'ROOT':\n",
        "                for child in word.children:\n",
        "                    if child.dep == 'nsubjpass':\n",
        "                        subject = child\n",
        "                        predicate = word.text + '-as'\n",
        "                        for gchild in word.children:\n",
        "                            if gchild.dep == 'attr':\n",
        "                                obj = gchild\n",
        "                                triples.append((subject.text, predicate, obj.text))\n",
        "                            elif gchild.dep == 'prep':\n",
        "                                preposition = gchild.text\n",
        "                                for ggchild in gchild.children:\n",
        "                                    if ggchild.dep == 'pobj':\n",
        "                                        triples.append((subject.text, word.text + '-' + preposition, ggchild.text))\n",
        "    return triples\n",
        "\n",
        "# function call\n",
        "passive_triples = extract_passive_triples(sentences_with_dependencies)\n",
        "print(passive_triples[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xgkcCXIVY_4",
        "outputId": "548c71ac-d91c-4e14-cbbc-064b36f1c3de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 48. Extract paths from the root to nounsPermalink\n",
        "For every noun in a dependency tree, extract a path from the root to the noun. Here, each path must satisfy the following specifications.\n",
        "\n",
        "Nodes in a path are words in surface form\n",
        "Nodes are connected with “ -> “ from the root to the leaf node\n",
        "We don’t have to include dependency types (e.g., nsubj, dobj) when representing a dependency path.\n",
        "For the example sentence, “Frank Rosenblatt invented the perceptron”, we expect an output,\n",
        "\n",
        "invented -> Rosenblatt\n",
        "invented -> Rosenblatt -> Frank\n",
        "invented -> perceptron"
      ],
      "metadata": {
        "id": "s0cCjuf0VlVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_shortest_paths_between_nouns(sentence):\n",
        "    def find_common_ancestor(word1, word2):\n",
        "        ancestors1 = set()\n",
        "        while word1:\n",
        "            ancestors1.add(word1)\n",
        "            word1 = word1.head\n",
        "        while word2:\n",
        "            if word2 in ancestors1:\n",
        "                return word2\n",
        "            word2 = word2.head\n",
        "        return None\n",
        "\n",
        "    def get_path(word, target):\n",
        "        path = []\n",
        "        while word and word != target:\n",
        "            path.append(word.text)\n",
        "            word = word.head\n",
        "        path.append(target.text)\n",
        "        return path\n",
        "\n",
        "    noun_indices = [i for i, word in enumerate(sentence) if word.pos.startswith('NN')]\n",
        "    paths = []\n",
        "\n",
        "    for i in range(len(noun_indices)):\n",
        "        for j in range(i+1, len(noun_indices)):\n",
        "            word1 = sentence[noun_indices[i]]\n",
        "            word2 = sentence[noun_indices[j]]\n",
        "            common_ancestor = find_common_ancestor(word1, word2)\n",
        "\n",
        "            if common_ancestor:\n",
        "                path1 = get_path(word1, common_ancestor)\n",
        "                path2 = get_path(word2, common_ancestor)\n",
        "                path = path1[:-1] + list(reversed(path2))\n",
        "                path[0] = 'X'\n",
        "                path[-1] = 'Y'\n",
        "                paths.append(' -> '.join(path))\n",
        "    return paths\n",
        "\n",
        "# Example usage\n",
        "shortest_paths = extract_shortest_paths_between_nouns(sentences_with_dependencies[0])\n",
        "print(shortest_paths)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKVltNUCVv_s",
        "outputId": "d1067561-9986-4dba-e412-ed5bf6e5e63d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['X -> Y', 'X -> science -> called -> Y', 'X -> science -> called -> intelligence -> Y', 'X -> science -> called -> intelligence -> Y', 'X -> science -> called -> Y', 'X -> science -> called -> is -> Y', 'X -> science -> called -> is -> intelligence -> demonstrated -> Y', 'X -> science -> called -> is -> intelligence -> Y', 'X -> science -> called -> is -> intelligence -> contrast -> Y', 'X -> science -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> Y', 'X -> science -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> humans -> Y', 'X -> called -> Y', 'X -> called -> intelligence -> Y', 'X -> called -> intelligence -> Y', 'X -> called -> Y', 'X -> called -> is -> Y', 'X -> called -> is -> intelligence -> demonstrated -> Y', 'X -> called -> is -> intelligence -> Y', 'X -> called -> is -> intelligence -> contrast -> Y', 'X -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> Y', 'X -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> humans -> Y', 'X -> Y', 'X -> called -> intelligence -> Y', 'X -> called -> Y', 'X -> called -> is -> Y', 'X -> called -> is -> intelligence -> demonstrated -> Y', 'X -> called -> is -> intelligence -> Y', 'X -> called -> is -> intelligence -> contrast -> Y', 'X -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> Y', 'X -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> humans -> Y', 'X -> intelligence -> called -> intelligence -> Y', 'X -> intelligence -> called -> Y', 'X -> intelligence -> called -> is -> Y', 'X -> intelligence -> called -> is -> intelligence -> demonstrated -> Y', 'X -> intelligence -> called -> is -> intelligence -> Y', 'X -> intelligence -> called -> is -> intelligence -> contrast -> Y', 'X -> intelligence -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> Y', 'X -> intelligence -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> humans -> Y', 'X -> Y', 'X -> intelligence -> called -> is -> Y', 'X -> intelligence -> called -> is -> intelligence -> demonstrated -> Y', 'X -> intelligence -> called -> is -> intelligence -> Y', 'X -> intelligence -> called -> is -> intelligence -> contrast -> Y', 'X -> intelligence -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> Y', 'X -> intelligence -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> humans -> Y', 'X -> called -> is -> Y', 'X -> called -> is -> intelligence -> demonstrated -> Y', 'X -> called -> is -> intelligence -> Y', 'X -> called -> is -> intelligence -> contrast -> Y', 'X -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> Y', 'X -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> humans -> Y', 'X -> demonstrated -> Y', 'X -> Y', 'X -> contrast -> Y', 'X -> contrast -> intelligence -> displayed -> Y', 'X -> contrast -> intelligence -> displayed -> humans -> Y', 'X -> demonstrated -> intelligence -> Y', 'X -> demonstrated -> intelligence -> contrast -> Y', 'X -> demonstrated -> intelligence -> contrast -> intelligence -> displayed -> Y', 'X -> demonstrated -> intelligence -> contrast -> intelligence -> displayed -> humans -> Y', 'X -> Y', 'X -> intelligence -> displayed -> Y', 'X -> intelligence -> displayed -> humans -> Y', 'X -> displayed -> Y', 'X -> displayed -> humans -> Y', 'X -> Y']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 49. Extract the shortest path between two nouns\n",
        "Extract the shortest path for every pair of two nouns. Supposing that two nouns appear at the i\n",
        "-th and j\n",
        "-th positions (in words) in a sentence (i<j)\n",
        ", the shortest path must satisfy the following specifications.\n",
        "\n",
        "Nodes in a path are words in surface form\n",
        "Nodes corresponding to the i\n",
        "-th and j\n",
        "-th words are replaced with X and Y, respectively.\n",
        "Nodes are connected with either “ -> “ or “ <- “ from X to Y to represent a direction of a dependency.\n",
        "We can consider two types of dependency paths.\n",
        "\n",
        "When the j\n",
        "-th word appears on the path from the i\n",
        "-th word to the root: the path from the i\n",
        "-th word to the j\n",
        "-th word\n",
        "When the i\n",
        "-th and j\n",
        "-th words have the common ancestor (the k\n",
        "-th word) in the dependency tree: the path from the i\n",
        "-th word to the k\n",
        "-th word connected with “ <- “, followed by the path from the k\n",
        "-th word to the j\n",
        "-th word connected with “ -> “.\n",
        "For the example sentence, “Frank Rosenblatt invented the perceptron”, we expect an output,\n",
        "\n",
        "X <- Y\n",
        "X <- invented -> Y\n",
        "X <- Rosenblatt <- invented -> Y"
      ],
      "metadata": {
        "id": "HtZyHLXPV05C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_shortest_paths_between_nouns(sentence):\n",
        "    def find_common_ancestor(word1, word2):\n",
        "        ancestors1 = set()\n",
        "        while word1:\n",
        "            ancestors1.add(word1)\n",
        "            word1 = word1.head\n",
        "        while word2:\n",
        "            if word2 in ancestors1:\n",
        "                return word2\n",
        "            word2 = word2.head\n",
        "        return None\n",
        "\n",
        "    def get_path(word, target):\n",
        "        path = []\n",
        "        while word and word != target:\n",
        "            path.append(word.text)\n",
        "            word = word.head\n",
        "        path.append(target.text)\n",
        "        return path\n",
        "\n",
        "    noun_indices = [i for i, word in enumerate(sentence) if word.pos.startswith('NN')]\n",
        "    paths = []\n",
        "\n",
        "    for i in range(len(noun_indices)):\n",
        "        for j in range(i+1, len(noun_indices)):\n",
        "            word1 = sentence[noun_indices[i]]\n",
        "            word2 = sentence[noun_indices[j]]\n",
        "            common_ancestor = find_common_ancestor(word1, word2)\n",
        "\n",
        "            if common_ancestor:\n",
        "                path1 = get_path(word1, common_ancestor)\n",
        "                path2 = get_path(word2, common_ancestor)\n",
        "                path = path1[:-1] + list(reversed(path2))\n",
        "                path[0] = 'X'\n",
        "                path[-1] = 'Y'\n",
        "                paths.append(' -> '.join(path))\n",
        "    return paths\n",
        "\n",
        "# Example usage\n",
        "shortest_paths = extract_shortest_paths_between_nouns(sentences_with_dependencies[0])\n",
        "print(shortest_paths)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0itwkuDdV7zp",
        "outputId": "07fd3026-49b0-4978-ce74-0b39411bb23e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['X -> Y', 'X -> science -> called -> Y', 'X -> science -> called -> intelligence -> Y', 'X -> science -> called -> intelligence -> Y', 'X -> science -> called -> Y', 'X -> science -> called -> is -> Y', 'X -> science -> called -> is -> intelligence -> demonstrated -> Y', 'X -> science -> called -> is -> intelligence -> Y', 'X -> science -> called -> is -> intelligence -> contrast -> Y', 'X -> science -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> Y', 'X -> science -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> humans -> Y', 'X -> called -> Y', 'X -> called -> intelligence -> Y', 'X -> called -> intelligence -> Y', 'X -> called -> Y', 'X -> called -> is -> Y', 'X -> called -> is -> intelligence -> demonstrated -> Y', 'X -> called -> is -> intelligence -> Y', 'X -> called -> is -> intelligence -> contrast -> Y', 'X -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> Y', 'X -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> humans -> Y', 'X -> Y', 'X -> called -> intelligence -> Y', 'X -> called -> Y', 'X -> called -> is -> Y', 'X -> called -> is -> intelligence -> demonstrated -> Y', 'X -> called -> is -> intelligence -> Y', 'X -> called -> is -> intelligence -> contrast -> Y', 'X -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> Y', 'X -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> humans -> Y', 'X -> intelligence -> called -> intelligence -> Y', 'X -> intelligence -> called -> Y', 'X -> intelligence -> called -> is -> Y', 'X -> intelligence -> called -> is -> intelligence -> demonstrated -> Y', 'X -> intelligence -> called -> is -> intelligence -> Y', 'X -> intelligence -> called -> is -> intelligence -> contrast -> Y', 'X -> intelligence -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> Y', 'X -> intelligence -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> humans -> Y', 'X -> Y', 'X -> intelligence -> called -> is -> Y', 'X -> intelligence -> called -> is -> intelligence -> demonstrated -> Y', 'X -> intelligence -> called -> is -> intelligence -> Y', 'X -> intelligence -> called -> is -> intelligence -> contrast -> Y', 'X -> intelligence -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> Y', 'X -> intelligence -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> humans -> Y', 'X -> called -> is -> Y', 'X -> called -> is -> intelligence -> demonstrated -> Y', 'X -> called -> is -> intelligence -> Y', 'X -> called -> is -> intelligence -> contrast -> Y', 'X -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> Y', 'X -> called -> is -> intelligence -> contrast -> intelligence -> displayed -> humans -> Y', 'X -> demonstrated -> Y', 'X -> Y', 'X -> contrast -> Y', 'X -> contrast -> intelligence -> displayed -> Y', 'X -> contrast -> intelligence -> displayed -> humans -> Y', 'X -> demonstrated -> intelligence -> Y', 'X -> demonstrated -> intelligence -> contrast -> Y', 'X -> demonstrated -> intelligence -> contrast -> intelligence -> displayed -> Y', 'X -> demonstrated -> intelligence -> contrast -> intelligence -> displayed -> humans -> Y', 'X -> Y', 'X -> intelligence -> displayed -> Y', 'X -> intelligence -> displayed -> humans -> Y', 'X -> displayed -> Y', 'X -> displayed -> humans -> Y', 'X -> Y']\n"
          ]
        }
      ]
    }
  ]
}